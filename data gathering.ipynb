{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6238e82-b1b8-4b4a-8e63-b2c8092be0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests as req\n",
    "import http.cookiejar\n",
    "import urllib.request\n",
    "import time as tm\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e04f4aa-a128-4ca9-99a9-f652d6c8f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "cookies_path = os.getenv(\"cookies_path\")\n",
    "\n",
    "sys.path.append(os.getenv('scraper_path'))\n",
    "from YT_Scrapy import YtScraper\n",
    "scrapper = YtScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54b8b4b-0ec6-45e9-9429-cd87400009f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getenv('scraper_path'))\n",
    "from YT_Scrapy import YtScraper\n",
    "scrapper = YtScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131988d6-5492-4b59-bbe5-990e1c9a044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cookies_from_file(cookie_file):\n",
    "    cookie_jar = http.cookiejar.MozillaCookieJar(cookie_file)\n",
    "    cookie_jar.load()\n",
    "    return cookie_jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f911380-6aa5-4d74-a9bf-dc899574e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_file(path:str):\n",
    "    return os.path.isfile(path) #check if file is exist \n",
    "\n",
    "\n",
    "def save_csv(path:str, df):\n",
    "    # if file exist, joining existing csv file with new one then save\n",
    "    if check_existing_file(path): \n",
    "        old_df = pd.read_csv(path)\n",
    "        result = pd.concat([old_df, df], axis=0)\n",
    "        result.to_csv(path,index=False)\n",
    "        \n",
    "    # else save file directly\n",
    "    else:\n",
    "        df.to_csv(path,index=False)\n",
    "        \n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01bdc89-4a19-423a-95c4-bee5690a6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rank(df):\n",
    "    counter = 1\n",
    "    df['video_rank']=None\n",
    "    for i in range(df.shape[0]):\n",
    "        df['video_rank'].iat[i] = counter\n",
    "        counter +=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7201e0-537d-4fcd-a306-3b22f3b4af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_tracking(latest_data,data_path,file_name,times):\n",
    "    file_name = f'{data_path}{file_name}_reserve.csv'\n",
    "    if latest_data is not None:\n",
    "        if check_existing_file(file_name):\n",
    "            existing_data = pd.read_csv(file_name)\n",
    "            existing_data_ids = pd.Series(existing_data['video_id'].unique())#.tolist()\n",
    "            existing_data_ids = existing_data_ids[~existing_data_ids.isin(latest_data['video_id'])].tolist()\n",
    "            new_existing_data = scrapper.scrape_video_data(existing_data_ids)\n",
    "            new_existing_data['time_recorded'] = times\n",
    "            # latest_data = latest_data.loc[~latest_data['video_id'].isin(new_existing_data['video_id'])]\n",
    "            new_data = pd.concat([existing_data,new_existing_data,latest_data],axis=0)\n",
    "            # new_data = new_data.drop_duplicates(subset=['video_id', 'time_recorded'], keep='first')\n",
    "            new_data.to_csv(file_name,index=False)\n",
    "            del existing_data,new_existing_data,latest_data,new_data,existing_data_ids\n",
    "            return None\n",
    "        else:\n",
    "            latest_data.to_csv(file_name,index=False)\n",
    "            del latest_data\n",
    "            return None\n",
    "    else:\n",
    "        if not check_existing_file(file_name):\n",
    "            return None\n",
    "        else:\n",
    "            existing_data = pd.read_csv(file_name)\n",
    "            existing_data_ids = existing_data['video_id'].unique().tolist()\n",
    "            new_existing_data = scrapper.scrape_video_data(existing_data_ids)\n",
    "            new_existing_data['time_recorded'] = times\n",
    "            new_data = pd.concat([existing_data,new_existing_data],axis=0)\n",
    "            new_data.to_csv(file_name,index=False)\n",
    "            del new_data,new_existing_data,existing_data_ids,existing_data\n",
    "            return None\n",
    "        \n",
    "        \n",
    "def process_and_save(data_type, ids, data_path, file_name, counters, times):\n",
    "    if len(ids) > 0:\n",
    "        df = scrapper.scrape_video_data(sorted(ids))\n",
    "        df['time_recorded'] = times\n",
    "        video_tracking(df,data_path,file_name,times)\n",
    "        tm.sleep(10)\n",
    "        df = add_rank(df)\n",
    "        save_csv(f'{data_path}{file_name}.csv', df)\n",
    "        print(f'df {data_type} {counters}')\n",
    "        tm.sleep(10)\n",
    "    else:\n",
    "        video_tracking(None,data_path,file_name,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573a9a6d-d368-45e7-b6c6-2409c7b4353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = req.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecc006-8ef9-464d-ac8b-93be14924683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df video 1\n",
      "df video 2\n",
      "df video 3\n",
      "df video 4\n",
      "df video 5\n",
      "df video 6\n",
      "df video 7\n",
      "df video 8\n",
      "df video 9\n",
      "df video 10\n",
      "df video 11\n",
      "df video 12\n",
      "df video 13\n",
      "df video 14\n",
      "df video 15\n",
      "df video 16\n",
      "df video 17\n",
      "df video 18\n",
      "df video 19\n",
      "df video 20\n",
      "df video 21\n"
     ]
    }
   ],
   "source": [
    "# counters = 1\n",
    "# # keywords = ['vtuber','yoasobi','frieren','python','neuro sama','furina','genshin','honor of king']\n",
    "# for _ in range(100):\n",
    "#     # for keyword in keywords:\n",
    "#     cookie_jar = http.cookiejar.LWPCookieJar(cookies_path)\n",
    "#     session.cookies = cookie_jar\n",
    "#     times = dt.now()\n",
    "#     headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'}\n",
    "#     payload = {\n",
    "#         'search_query': 'vtuber',\n",
    "#         'sp':'EgQQAUAB'\n",
    "#     }\n",
    "#     html = session.get('https://www.youtube.com/results',params=payload,headers=headers)\n",
    "#     soup = BeautifulSoup(html.content, 'html.parser')\n",
    "#     yt_initial_data_match = re.search(r'var ytInitialData = ({.*?});', html.text)\n",
    "#     if yt_initial_data_match:\n",
    "#         yt_initial_data = json.loads(yt_initial_data_match.group(1))\n",
    "#     tmp_data = yt_initial_data.get('contents').get('twoColumnSearchResultsRenderer').get('primaryContents').get('sectionListRenderer').get('contents')\n",
    "#     for i in range(len(tmp_data)):\n",
    "#         tmp_data_1 = tmp_data[i].get('itemSectionRenderer').get('contents')\n",
    "#         if tmp_data_1:\n",
    "#             tmp_data = tmp_data_1\n",
    "#             del tmp_data_1\n",
    "#             break\n",
    "#         else:\n",
    "#             pass\n",
    "#     new_short_id = set()\n",
    "#     old_short_id = set()\n",
    "#     video_id = set()\n",
    "    \n",
    "#     for i in range(len(tmp_data)):\n",
    "#         for key in tmp_data[i].keys():\n",
    "#             if key == 'reelShelfRenderer' and tmp_data[i].get(key).get('title').get('simpleText') == 'Shorts':\n",
    "#                 if len(tmp_data[i].get(key).get('items')) > 0:\n",
    "#                     for x in range(len(tmp_data[i].get(key).get('items'))):\n",
    "#                         data_holder = tmp_data[i].get(key).get('items')[x].get('shortsLockupViewModel').get('onTap').get('innertubeCommand')\n",
    "#                         old_short_id.add(data_holder.get('reelWatchEndpoint').get('videoId'))\n",
    "#                         del data_holder\n",
    "                        \n",
    "#             elif key == 'reelShelfRenderer' and tmp_data[i].get(key).get('title').get('simpleText') == 'Recently uploaded Shorts':\n",
    "#                 if len(tmp_data[i].get(key).get('items')) > 0:\n",
    "#                     for x in range(len(tmp_data[i].get(key).get('items'))):\n",
    "#                         data_holder = tmp_data[i].get(key).get('items')[x].get('shortsLockupViewModel').get('onTap').get('innertubeCommand')\n",
    "#                         new_short_id.add(data_holder.get('reelWatchEndpoint').get('videoId'))\n",
    "#                         del data_holder\n",
    "                        \n",
    "#             elif key == 'videoRenderer':\n",
    "#                 video_id.add(tmp_data[i].get(key).get('videoId'))\n",
    "#     data_path = 'data/youtube_scraping/'\n",
    "#     # process_and_save('short old', old_short_id,data_path, 'old_short', counters, times)\n",
    "    \n",
    "    \n",
    "#     # Process new shorts\n",
    "#     # process_and_save('short new', new_short_id,data_path, 'new_short', counters, times)\n",
    "    \n",
    "#     # Process videos\n",
    "#     process_and_save('video', video_id,data_path, 'video', counters, times)\n",
    "    \n",
    "#     # Save cookie and update counters\n",
    "#     cookie_jar.save(ignore_discard=True, ignore_expires=True)\n",
    "#     counters += 1\n",
    "#     tm.sleep(60 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e39267d-4808-40da-9b27-c9291628d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-09-04'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_date = datetime.date(2023, 9, 4)\n",
    "start_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46a143-1703-47f9-a912-c1a8983b1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = datetime.date(2023, 9, 4)\n",
    "end_date = datetime.date.today()\n",
    "end_date -= datetime.timedelta(days=1)\n",
    "# Inisialisasi loop\n",
    "current_date = start_date\n",
    "date_list = []\n",
    "data_path = 'data/youtube_scraping/'\n",
    "# Loop dari tanggal start_date hingga end_date\n",
    "while current_date <= end_date:\n",
    "    # Format tanggal menjadi string yyyy-mm-dd\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    date_list.append(date_str)\n",
    "    \n",
    "    # Tambahkan satu hari ke tanggal saat ini\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    tomorow_date = current_date.strftime('%Y-%m-%d')\n",
    "    df = scrapper.scrape_search_video(\n",
    "        q='debut stream vtuber',\n",
    "        regionCode='id',\n",
    "        publishedAfter=date_str,\n",
    "        publishedBefore=tomorow_date,\n",
    "        all_data=True\n",
    "    )\n",
    "\n",
    "    save_csv(f'{data_path}vtuber_debut.csv', df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
